# ğŸ§  Simple Word Based Tokenizer

A minimalistic Python script that reads a text file, builds a vocabulary from it, and tokenizes user input text into integer sequences. It also supports decoding those sequences back into human-readable text.

---

## ğŸ“Œ Features

- ğŸ“– Reads and preprocesses text from a user-specified file (e.g., `the-verdict.txt`)
- âœ‚ï¸ Tokenizes text using regular expressions (words + punctuation)
- ğŸ”¡ Creates a vocabulary from unique tokens in the file
- ğŸ” Encodes text to integer IDs and decodes back to readable text
- â“ Handles unknown tokens using `<|unk|>`
- ğŸ“ Uses special tokens like `<|endoftext|>` to separate segments

---

## ğŸ› ï¸ How It Works

1. **Preprocessing**: Text is split using punctuation and spaces via regex.
2. **Vocabulary Generation**: A sorted set of all unique tokens is mapped to integers.
3. **Encoding**: User inputs are tokenized and converted to integer IDs.
4. **Decoding**: ID sequences are translated back into readable text with proper punctuation formatting.

---

## ğŸš€ Usage

### ğŸ“‚ Step 1: Prepare Your Text File
Place your input file (e.g., `the-verdict.txt`) in the same directory or provide the full path.

### â–¶ï¸ Step 2: Run the Script

```bash
python tokenizer.py
